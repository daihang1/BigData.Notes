# Hadoop:

**hdfs的写流程：**

1）跟NN通信请求上传文件，NN检查目标文件是否存在，父目录是否存在

2）NN返回是否可以上传

3）client会先对文件进行切分，比如一个block块128M，文件300M就会被切分成3个块，两个128M，一个44M。请求第一个block该传输到哪些DN服务器上。

4）NN返回DN的服务器。

5）client请求一个台DN上传数据（RPC调用，建立pipeline），第一个DN收到请求会继续调用第二个DN，然后第二个调用第三个DN,将整个pipeline建立完成，逐级返回客户端。

6）client开始传输block(先从磁盘读取数据存储到一个本地内存缓存)，以packet为单位（一个packet为64kb）,写入数据的时候datanode会进行数据校验，并不是通过packet为单位校验，而是以chunk为单位校验（512byte）,第一个DN收到第一个packet就会传给第二台，第二台传给第三台；第一台每传一个packet就会放入一个应答队列等待应答。

7）当一个block传输完成时，client再次请求NN上传第二个block的服务器。

**hdfs的读流程：**

1）跟NN通信查询元数据(block所在的DN的节点)，找到文件块所在的DN的服务器。

2）挑选一台DN（就近原则，然后随机）服务器，请求建立socket流。

3）DN开始发送数据（从磁盘里读取数据放入流，一packet为单位做校验）

4）客户端以packet为单位接收，现在本地缓存，然后写入目标文件中，后面的block块就相当于append到前面的block块，最后合成最终需要的文件。

hdfs创建一个文件的流程：

1. 客户端通过ClientProtocol协议向RpcServer发起创建文件的RPC请求。
2. FSNamesystem封装了各种HDFS操作的实现细节，RpcServer调用FSNamesystem中的相关方法以创建目录。
3. 进一步的，FSDirectory封装了各种目录树操作的实现细节，FSNamesystem调用FSDirectory中的相关方法在目录树中创建目标文件，并通过日志系统备份文件系统的修改。
4. 最后，RpcServer将RPC响应返回给客户端。

hadoop1.x和hadoop2.x的区别：

1. **资源调度方式的改变**

   在1.x, 使用Jobtracker负责任务调度和资源管理,单点负担过重,在2.x中,新增了yarn作为集群的调度工具.在yarn中,使用ResourceManager进行 资源管理, 单独开启一个Container作为ApplicationMaster来进行任务管理.

2. **HA模式**

   在1.x中没有HA模式,集群中只有一个NameNode,而在2.x中可以启用HA模式,存在一个Active NameNode 和Standby NameNode.

3. **HDFS Federation**

   Hadoop 2.0中对HDFS进行了改进，使NameNode可以横向扩展成多个，每个NameNode分管一部分目录，进而产生了HDFS Federation，该机制的引入不仅增强了HDFS的扩展性，也使HDFS具备了隔离性

hadoop的常用配置文件有哪些：



- **hadoop-env.sh**: 用于定义hadoop运行环境相关的配置信息，比如配置JAVA_HOME环境变量、为hadoop的JVM指定特定的选项、指定日志文件所在的目录路径以及master和slave文件的位置等；
- **core-site.xml**: 用于定义系统级别的参数，如HDFS URL、Hadoop的临时目录以及用于rack-aware集群中的配置文件的配置等，此中的参数定义会覆盖core-default.xml文件中的默认配置；
- **hdfs-site.xml**: HDFS的相关设定，如文件副本的个数、块大小及是否使用强制权限等，此中的参数定义会覆盖hdfs-default.xml文件中的默认配置；
- **mapred-site.xml**：HDFS的相关设定，如reduce任务的默认个数、任务所能够使用内存的默认上下限等，此中的参数定义会覆盖mapred-default.xml文件中的默认配置；



**hadoop的优化：**

​     1)HDFS小文件影响

​          a.影响NameNode的寿命，因为文件元数据存储在NameNode的内存中

​          b.影响计算引擎的任务数量，比如每个小的文件都会生成一个Map任务

​     2)数据输入小文件处理：

​          a.合并小文件：对小文件进行归档（Har）、自定义Inputformat将小文件存储成SequenceFile文件。

​          b.采用ConbinFileInputFormat来作为输入，解决输入端大量小文件场景。

​          c.对于大量小文件Job，可以开启JVM重用。

​     3)Map阶段

​          a.增大环形缓冲区大小。由100m扩大到200m

​          b.增大环形缓冲区溢写的比例。由80%扩大到90%

​          c.减少对溢写文件的merge次数。

​          d.不影响实际业务的前提下，采用Combiner提前合并，减少 I/O。

​     4)Reduce阶段

​          a.合理设置Map和Reduce数：两个都不能设置太少，也不能设置太多。太少，会导致Task等待，延长处理时间；太多，会导致 Map、Reduce任务间竞争资源，造成处理超时等错误。

​          b.设置Map、Reduce共存：调整[slowstart.completedmaps](http://slowstart.completedmaps/)参数，使Map运行到一定程度后，Reduce也开始运行，减少Reduce的等待时间。

​          c.规避使用Reduce，因为Reduce在用于连接数据集的时候将会产生大量的网络消耗。

​          d.增加每个Reduce去Map中拿数据的并行数

​          e.集群性能可以的前提下，增大Reduce端存储数据内存的大小。

​     5)IO传输

​          a.采用数据压缩的方式，减少网络IO的的时间。安装Snappy和LZOP压缩编码器。

​          b.使用SequenceFile二进制文件

​     6)整体

​          a.MapTask默认内存大小为1G，可以增加MapTask内存大小为4-5g

​          b.ReduceTask默认内存大小为1G，可以增加ReduceTask内存大小为4-5g

​          c.可以增加MapTask的cpu核数，增加ReduceTask的CPU核数

​          d.增加每个Container的CPU核数和内存大小

​          e.调整每个Map Task和Reduce Task最大重试次数

简答说一下hadoop的map-reduce编程模型:

首先map task会从本地文件系统读取数据，转换成key-value形式的键值对集合

使用的是hadoop内置的数据类型，比如longwritable、text等

将键值对集合输入mapper进行业务处理过程，将其转换成需要的key-value在输出

之后会进行一个partition分区操作，默认使用的是hashpartitioner，可以通过重写hashpartitioner的getpartition方法来自定义分区规则

之后会对key进行进行sort排序，grouping分组操作将相同key的value合并分组输出，在这里可以使用自定义的数据类型，重写WritableComparator的Comparator方法来自定义排序规则，重写RawComparator的compara方法来自定义分组规则

之后进行一个combiner归约操作，其实就是一个本地段的reduce预处理，以减小后面shufle和reducer的工作量

reduce task会通过网络将各个数据收集进行reduce处理，最后将数据保存或者显示，结束整个job

hadoop的TextInputFormat作用是什么，如何自定义实现:

InputFormat会在map操作之前对数据进行两方面的预处理 

1是getSplits，返回的是InputSplit数组，对数据进行split分片，每片交给map操作一次 

2是getRecordReader，返回的是RecordReader对象，对每个split分片进行转换为key-value键值对格式传递给map

常用的InputFormat是TextInputFormat，使用的是LineRecordReader对每个分片进行键值对的转换，以行偏移量作为键，行内容作为值

自定义类继承InputFormat接口，重写createRecordReader和isSplitable方法 

在createRecordReader中可以自定义分隔符

**hadoop实现join的几种方式：**

**1、在reducer端进行join：**map端同时读取两个文件进行打标签，在 reducer端把两个不同文件的key相同的value进行join（笛卡尔积），即reducer端 进行实际的join，会产生大量的shuffle，低效的 。

**2、在map端进行join：**是针对两个待连接表，一个大表和一个小表，小表非常小，足够直接存放在内存中。这样我们就可以复制多份，让每个map的task内存中存在一份（比如存放到hash table中），然后只扫描大表，对于大表 中每一条记录key/value，在hashtable中查找是否有相同的key的记录，如果有，则连接后输出。

**3、SemiJoin（半连接）：**reducer端的join，跨机器传输数据，会产生大量的磁盘io，成为限制join的瓶颈，在map阶段过滤。实现方法是：选取一个小表f1，将其参于join的key取出来，保存到f3中，f3文件一般很小，可以存放到内存中。在 map阶段，使用DistributedCache(分布式缓存)将f3的数据复制到各个taskTracker上，然后将f2中与f3数据不匹配的key所对应的的记录过滤掉，剩下的reduce阶段工作与reducer端join相同。

**hadoop的DistributedCache（分布式缓存）：（map端进行join）**

​    DistributedCache是hadoop框架提供的一种机制,可以将job指定的文件,在job执行前,先行分发到task执行的机器上,并有相关机制对cache文件进行管理.

​       DistributedCache 可将具体应用相关的、大尺寸的、只读的文件有效地分布放置。DistributedCache 是Map/Reduce框架提供的功能，能够缓存应用程序所需的文件 （包括文本，档案文件，jar文件等）。

​       可以将小表分发到所有的map节点，这样，map节点就可以在本地。对自己所读到的大表数据进行join并输出最终结果，可以大大提高join操作的并发度，加快处理速度；

 Hadoop提供了一个Distributed Cache机制，能把文件在合适的时候发给MapTask，MapTask就可以从本地进行加载小表数据；

**CombineTextInputFormat(切片机制优化大量小文件)：**

 将hdfs上多个小文件合并到一个InputSplit中，然后会启用一个map来处理这里面的文件，以此减少mr整体作业人的运行时间。

  **CombineTextInputFormat.setMaxInputSplitSize(job, 4194304)**

  CombineTextInputFormat用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个MapTask处理



​        combineTextInputFormat分为虚拟储存过程和切片过程：

​        虚拟储存过程： 根据是否设置setMaxInputSplitSize值，将每个文件划分成一个一个setMaxInputSplitSize值大小的文件

​      当剩余数据大小超过setMaxInputSplitSize值且不大于2倍setMaxInputSplitSize值，此时将文件均分成2个虚拟存储块（防止出现太小切片)

​        切片过程：断虚拟存储的文件大小是否大于setMaxInputSplitSize值，大于等于则单独形成一个切片如果不大于则跟下一个虚拟存储文             件进行合并，共同形成一个切片

hadoop的切片机制：

​    简单的按照文件的内容长度进行切片

​      切片大小，默认等于Block大小

​      切片时不考虑数据集的整体，而是逐个针对每一个文件单独切片

​    源码中的切片计算公式： Math.max(minSize, Math.min(maxSize,blockSize))



**yarn的工作机制：**

（0）Mr 程序提交到客户端所在的节点。

（1）Yarnrunner 向 Resourcemanager 申请一个 Application。

（2）rm 将该应用程序的资源路径返回给 yarnrunner。

（3）该程序将运行所需资源提交到 HDFS 上。

（4）程序资源提交完毕后，申请运行 mrAppMaster。

（5）RM 将用户的请求初始化成一个 task。

（6）其中一个 NodeManager 领取到 task 任务。

（7）该 NodeManager 创建容器 Container，并产生 MRAppmaster。

（8）Container 从 HDFS 上拷贝资源到本地。

（9）MRAppmaster 向 RM 申请运行 maptask 资源。

（10）RM 将运行 maptask 任务分配给另外两个 NodeManager，另两个 NodeManager 分

别领取任务并创建容器。

（11）MR 向两个接收到任务的 NodeManager 发送程序启动脚本，这两个 NodeManager

分别启动 maptask，maptask 对数据分区排序。

（12）MrAppMaster 等待所有 maptask 运行完毕后，向 RM 申请容器，运行 reduce task。

（13）reduce task 向 maptask 获取相应分区的数据。

（14）程序运行完毕后，MR 会向 RM 申请注销自己。

hadoop参数调优：

1. 在hdfs-site.xml文件中配置多目录，最好提前配置好，否则更改目录需要重新启动集群
2. NameNode有一个工作线程池，用来处理不同DataNode的并发心跳以及客户端并发的元数据操作。

dfs.namenode.handler.count=20 * log2(Cluster Size)，比如集群规模为10台时，此参数设置为60

1. 编辑日志存储路径dfs.namenode.edits.dir设置与镜像文件存储路径dfs.namenode.name.dir尽量分开，达到最低写入延迟
2. 服务器节点上YARN可使用的物理内存总量，默认是8192（MB），注意，如果你的节点内存资源不够8GB，则需要调减小这个值，而YARN不会智能的探测节点的物理内存总量。yarn.nodemanager.resource.memory-mb
3. 单个任务可申请的最多物理内存量，默认是8192（MB）。yarn.scheduler.maximum-allocation-mb

hadoop宕机：

1. 如果MR造成系统宕机。此时要控制Yarn同时运行的任务数，和每个任务申请的最大内存。调整参数：yarn.scheduler.maximum-allocation-mb（单个任务可申请的最多物理内存量，默认是8192MB）
2. 如果写入文件过量造成NameNode宕机。那么调高Kafka的存储大小，控制从Kafka到HDFS的写入速度。高峰期的时候用Kafka进行缓存，高峰期过去数据同步会自动跟上。

hadoop和spark的都是并行计算，那么他们有什么相同和区别:

两者都是用mr模型来进行并行计算，hadoop的一个作业称为job，job里面分为map task和reduce task，每个task都是在自己的进程中运行的，当task结束时，进程也会结束

spark用户提交的任务成为application，一个application对应一个sparkcontext，app中存在多个job，每触发一次action操作就会产生一个job

这些job可以并行或串行执行，每个job中有多个stage，stage是shuffle过程中DAGSchaduler通过RDD之间的依赖关系划分job而来的，每个stage里面有多个task，组成taskset有TaskSchaduler分发到各个executor中执行，executor的生命周期是和app一样的，即使没有job运行也是存在的，所以task可以快速启动读取内存进行计算

hadoop的job只有map和reduce操作，表达能力比较欠缺而且在mr过程中会重复的读写hdfs，造成大量的io操作，多个job需要自己管理关系

spark的迭代计算都是在内存中进行的，API中提供了大量的RDD操作如join，groupby等，而且通过DAG图可以实现良好的容错

# flume:

**为什么要用flume导入hdfs，hdfs的构架是怎样的:**

flume可以实时的导入数据到hdfs中，当hdfs上的文件达到一个指定大小的时候会形成一个文件，或者超过指定时间的话也形成一个文件

文件都是存储在datanode上面的，namenode记录着datanode的元数据信息，而namenode的元数据信息是存在内存中的，所以当文件切片很小或者很多的时候会卡死

简单说一下hadoop和spark的shuffle过程:

hadoop：map端保存分片数据，通过网络收集到reduce端 

spark：spark的shuffle是在DAGSchedular划分Stage的时候产生的，TaskSchedule要分发Stage到各个worker的executor

减少shuffle可以提高性能

**flum的负载均衡和故障转移机制：**

​    **负载均衡(load-balance)**是用于解决一台机器(一个进程)无法解决所有请求而产生的一种算法。**Load balancing Sink Processor**能够实现load balance功能，如下图Agent1是一个路由节点，负责将Channel暂存的Event均衡到对应的多个Sink组件上，而每个Sink组件分别连接到一个独立的Agent上，示例配置。

**Failover Sink Processor（故障转移机制）**维护一个优先级Sink组件列表，只要有一个Sink组件可用，Event就被传递到下一个组件。故障转移机制的作用是将失败的Sink降级到一个池，在这些池中它们被分配一个冷却时间，随着故障的连续，在重试之前冷却时间增加。一旦Sink成功发送一个事件，它将恢复到活动池。 Sink具有与之相关的优先级，数量越大，优先级越高。

**flume的组成：（put事物和task事物）**

​    

1. flume组成，Put事务，Take事务

Taildir Source：断点续传、多目录。Flume1.6以前需要自己自定义Source记录每次读取文件位置，实现断点续传。

File Channel：数据存储在磁盘，宕机数据可以保存。但是传输速率慢。适合对数据传输可靠性要求高的场景，比如，金融行业。

Memory Channel：数据存储在内存中，宕机数据丢失。传输速率快。适合对数据传输可靠性要求不高的场景，比如，普通的日志数据。

Kafka Channel：减少了Flume的Sink阶段，提高了传输效率。          

Source到Channel是Put事务

  Channel到Sink是Take事务

**flume自定义拦截器实现**

​    

1. 1. 实现 Interceptor
   2. 重写四个方法

- - initialize 初始化
  - public Event intercept(Event event) 处理单个Event
  - public List<Event> intercept(List<Event> events) 处理多个Event，在这个方法中调用Event intercept(Event event)
  - close 方法

**flume channel 选择器：**

​    channel selectors有两种类型：

​          Replicating Channel Selector (default):会将source过来的events发往所有的channel。

​          Multiplexing Channel Selector:可以选择发往那些channel。

**flume采集数据不会丢失，因为channel储存可以储存在File中，数据传输自身有事物。**

**Flume内存：**

​    开发中在flume-env.sh中设置JVM heap为4G或更高，部署在单独的服务器上（4核8线程16G内存）

  -Xmx（设定程序运行期间最大可占用的内存大小）与-Xms（设定程序启动时占用内存大小）最好设置一致，减少内存抖动带来的性能影响，如果设置不一致容易导致频繁fullgc。

FileChannel优化：

​      通过配置dataDirs指向多个路径，每个路径对应不同的硬盘，增大Flume吞吐量。

   checkpointDir和backupCheckpointDir也尽量配置在不同硬盘对应的目录中，保证checkpoint坏掉后，可以快速使用backupCheckpointDir恢复数据

HDFSSink小文件处理：

官方默认的这三个参数配置写入HDFS后会产生小文件，hdfs.rollInterval、hdfs.rollSize、hdfs.rollCount

基于以上hdfs.rollInterval=3600，hdfs.rollSize=134217728，hdfs.rollCount =0，hdfs.roundValue=10，hdfs.roundUnit= second几个参数综合作用，效果如下：

（1）tmp文件在达到128M时会滚动生成正式文件

（2）tmp文件创建超10分时会滚动生成正式文件

举例：在2018-01-01 05:23的时侯sink接收到数据，那会产生如下tmp文件：

/itcast/20180101/itcast.201801010520.tmp

即使文件内容没有达到128M，也会在05:33时滚动生成正式文件

# Kafka

kafka主要组件说明：kafka的脏数据一般在下游程序进行处理。

​    Kafka 有着默认的分区机制：

- - 如果键值为 null， 则使用轮询 (Round Robin) 算法将消息均衡地分布到各个分区上；
  - 如果键值不为 null，那么 Kafka 会使用内置的散列算法对键进行散列，然后分布到各个分区上。

​    producer:生产者，生产的消息通过topic进行归类，保存到kafka的broker里面去。

​     topic：主题，一个主题可以有零个，一个或多个消费者订阅写入的数据。

​     partition：分区，一般分区不要超过集群的机器数量（一般为3-10个）broker数与分区数没有关系； 在kafka中，每一个分区会有一个编号：编号从0开始每一个分区的数据是有序的说明-数据是有序 如何保证一个主题下的数据是有序的？（生产是什么样的顺序，那么消费的时候也是什么样的顺序）。

  partition的副本数：一般情况下等于broker的个数，一个broker服务下，不可以创建多个副本因子，副本因子应该小于或等于可赢得broker数。

  kafka分区与消费者组的关系：消费者组中的消费者数应该小于或等于该主题下的分区数。

kafka高吞吐、低延时、高性能的实现原理：

**一、页缓存技术+磁盘顺序写。**

**二、零拷贝技术。**

<https://blog.csdn.net/xiaoguozi0218/article/details/93310587>

## Kafka消息分发：

Producer客户端负责消息的分发



kafka集群中的任何一个broker都可以向producer提供metadata信息,这些metadata中包含”集群中存活的servers列表”/”partitions leader列表”等信息；

当producer获取到metadata信息之后, producer将会和Topic下所有partition

leader保持socket连接；

消息由producer直接通过socket发送到broker，中间不会经过任何”路由层”，事实上，消息被路由到哪个partition上由producer客户端决定；

比如可以采用”random”“key-hash”“轮询”等,如果一个topic中有多个partitions,那么在producer端实现”消息均衡分发”是必要的。

在producer端的配置文件中,开发者可以指定partition路由的方式。

## **kafka的数据丢失与重复：**

Kafka 的每一条消息都有一个偏移量属性，记录了其在分区中的位置，偏移量是一个单调递增的整数。消费者通过往一个叫作 ＿consumer_offset 的特殊主题发送消息，消息里包含每个分区的偏移量。 如果消费者一直处于运行状态，那么偏移量就没有 什么用处。不过，如果有消费者退出或者新分区加入，此时就会触发再均衡。完成再均衡之后，每个消费者可能分配到新的分区，而不是之前处理的那个。为了能够继续之前的工作，消费者需要读取每个分区最后一次提交的偏移量，然后从偏移量指定的地方继续处理。 因为这个原因，所以如果不能正确提交偏移量，就可能会导致数据丢失或者重复出现消费，比如下面情况：

- 如果提交的偏移量小于客户端处理的最后一个消息的偏移量 ，那么处于两个偏移量之间的消息就会被重复消费；
- 如果提交的偏移量大于客户端处理的最后一个消息的偏移量，那么处于两个偏移量之间的消息将会丢失。

**ack=0**:生产者消息发送后立即返回，不用确认消息是否发送成功。（性能最好，可靠性最差。发过去就完事了，不关心broker是否处理成功，可能丢数据。）

**ack=1**:（默认）生产者消息发送后，等待leader写入成功返回ack,则生产者才返回。（性能和可靠性相对平衡。当写Leader成功后就返回,其他的replica都是通过fetcher去同步的,所以kafka是异步写，主备切换可能丢数据。）

**ack=-1**:生产者消息发送后，不仅leader写入成功，还需要其他节点分区写入成功后，生产者才返回。（性能最差，可靠性最好。要等到isr里所有机器同步成功，才能返回成功，延时取决于最慢的机器。强一致，不会丢数据。）

注意：若ack=-1时还有数据丢失情况，确认是否集群中只有一台leader。

kafka中的CAP问题：

 分布式系统要遵守cap原则，即数据一致性，数据可用性，和分区容错性。

  其中**kafka分区容错性**为了解决kafka中的副本分区当中的数据与leader当中的数据存在差别的问题：

​    使用了ISR（基本同步列表）的同步策略：决定哪些副本分区是可用的，也就是说可以将leader分区里面的数据同步到副本分区里面去。决定一个副本分区是否可用的条件有两个：

​    

- replica.lag.time.max.ms=10000     副本分区与主分区心跳时间延迟
- ​      replica.lag.max.messages=4000    副本分区与主分区消息同步最大差
- **ISR（In-Sync Replicas）**，副本同步队列。ISR中包括Leader和Follower。如果Leader进程挂掉，会在ISR队列中选择一个服务作为新的Leader。有replica.lag.max.messages（延迟条数）和replica.lag.time.max.ms（延迟时间）两个参数决定一台服务是否可以加入ISR副本队列，在0.10版本移除了replica.lag.max.messages参数，防止服务频繁的进去队列。
- 任意一个维度超过阈值都会把Follower剔除出ISR，存入OSR（Outof-Sync Replicas）列表，新加入的Follower也会先存放在OSR中。
- 每个分区都有一个 ISR(in-sync Replica) 列表，用于维护所有同步的、可用的副本。首领副本必然是同步副本，而对于跟随者副本来说，它需要满足以下条件才能被认为是同步副本：
- 与 Zookeeper 之间有一个活跃的会话，即必须定时向 Zookeeper 发送心跳；
- 在规定的时间内从首领副本那里低延迟地获取过消息。
- 如果副本不满足上面条件的话，就会被从 ISR 列表中移除，直到满足条件才会被再次加入。
- 这里给出一个主题创建的示例：使用 --replication-factor 指定副本系数为 3，创建成功后使用 --describe 命令可以看到分区 0 的有 0,1,2 三个副本，且三个副本都在 ISR 列表中，其中 1 为首领副本。

kafka挂掉：

1. Flume记录
2. 日志有记录
3. 短期没事



## **Consumer的负载均衡:**

当一个group中,有consumer加入或者离开时,会触发partitions均衡.均衡的最终目的,是提升topic的并发消费能力，步骤如下：



假如topic1,具有如下partitions: P0,P1,P2,P3

加入group中,有如下consumer: C1,C2

首先根据partition索引号对partitions排序: P0,P1,P2,P3

根据consumer.id排序: C0,C1

计算倍数: M = [P0,P1,P2,P3].size / [C0,C1].size,本例值M=2(向上取整)

然后依次分配partitions: C0 = [P0,P1],C1=[P2,P3],即Ci = [P(i * M),P((i + 1)M -1)]

**Kafka消息数据积压，Kafka消费能力不足怎么处理：**

1. 如果是Kafka消费能力不足，则可以考虑增加Topic的分区数，并且同时提升消费组的消费者数量，消费者数=分区数。（两者缺一不可）
2. 如果是下游的数据处理不及时：提高每批次拉取的数量。批次拉取数据过少（拉取数据/处理时间<生产速度），使处理的数据小于生产的数据，也会造成数据积压。

kafka的再平衡机制：

**再平衡就是kafka consumer所订阅的topic发生变化时发生的一种分区重分配机制，一般有三种情况会触发 再平衡。**

- consumer group中的新增或删除某个consumer，导致其所消费的分区需要分配到组内其他的consumer上；
- consumer订阅的topic发生变化，比如订阅的topic采用的是正则表达式的形式，如test-*此时如果有一个新建了一个topic test-user，那么这个topic的所有分区也是会自动分配给当前的consumer的，此时就会发生再平衡；
- consumer所订阅的topic发生了新增分区的行为，那么新增的分区就会分配给当前的consumer，此时就会触发再平衡。

 **Kafka提供的再平衡策略主要有三种：****Round Robin****，****Range****和****Sticky****，默认使用的是****Range****。这三种分配策略的主要区别在于：**

​    

- Round Robin：会采用轮询的方式将当前所有的分区依次分配给所有的consumer；

- Range：首先会计算每个consumer可以消费的分区个数，然后按照顺序将指定个数范围的分区分配给各个consumer；

- Sticky：这种分区策略是最新版本中新增的一种策略，其主要实现了两个目的：

- - 将现有的分区尽可能均衡的分配给各个consumer，存在此目的的原因在于Round Robin和Range分配策略实际上都会导致某几个consumer承载过多的分区，从而导致消费压力不均衡；
  - 如果发生再平衡，那么重新分配之后在前一点的基础上会尽力保证当前未宕机的consumer所
  - 消费的分区不会被分配给其他的consumer上；

consumer如何通过 offset寻找数据：

如果consumer要找offset是1008的消息，那么，

1，按照二分法找到小于1008的segment，也就是00000000000000001000.log和00000000000000001000.index

2，用目标offset减去文件名中的offset得到消息在这个segment中的偏移量。也就是1008-1000=8，偏移量是8。

3，再次用二分法在index文件中找到对应的索引，也就是第三行6,45。

4，到log文件中，从偏移量45的位置开始（实际上这里的消息offset是1006），顺序查找，直到找到offset为1008的消息。查找期间kafka是按照log的存储格式来判断一条消息是否结束的。

kafka如何清理过期数据：



- #### **删除**

  log.cleanup.policy=delete启用删除策略

- - 直接删除，删除后的消息不可恢复。可配置以下两个策略： 清理超过指定时间清理：

    log.retention.hours=16

  - 超过指定大小后，删除旧的消息： log.retention.bytes=1073741824 为了避免在删除时阻塞读操作，采用了copy-on-write形式的实现，删除操作进行时，读取操作的二分查找功能实际是在一个静态的快照副本上进行的，这类似于Java的CopyOnWriteArrayList。

- #### **压缩**

  将数据压缩，只保留每个key最后一个版本的数据。 首先在broker的配置中设置log.cleaner.enable=true启用cleaner，这个默认是关闭的。 在topic的配置中设置log.cleanup.policy=compact启用压缩策略。

  [![img](https://github.com/CheckChe0803/BigData-Interview/raw/master/pictures/kafka%E5%8E%8B%E7%BC%A9.png)](https://github.com/CheckChe0803/BigData-Interview/blob/master/pictures/kafka压缩.png)

  如上图，在整个数据流中，每个Key都有可能出现多次，压缩时将根据Key将消息聚合，只保留最后一次出现时的数据。这样，无论什么时候消费消息，都能拿到每个Key的最新版本的数据。 压缩后的offset可能是不连续的，比如上图中没有5和7，因为这些offset的消息被merge了，当从这些offset消费消息时，将会拿到比这个offset大的offset对应的消息，比如，当试图获取offset为5的消息时，实际上会拿到offset为6的消息，并从这个位置开始消费。 这种策略只适合特俗场景，比如消息的key是用户ID，消息体是用户的资料，通过这种压缩策略，整个消息集里就保存了所有用户最新的资料。 压缩策略支持删除，当某个Key的最新版本的消息没有内容时，这个Key将被删除，这也符合以上逻辑。

zookeeper在kafka中的作用：



1. kafka的元数据都存放在zk上面,由zk来管理
2. 0.8之前版本的kafka, consumer的消费状态，group的管理以及 offset的值都是由zk管理的,现在offset会保存在本地topic文件里
3. 负责borker的lead选举和管理





kafka集群：

Kafka 使用 Zookeeper 来维护集群成员 (brokers) 的信息。每个 broker 都有一个唯一标识 broker.id，用于标识自己在集群中的身份，可以在配置文件 server.properties 中进行配置，或者由程序自动生成。下面是 Kafka brokers 集群自动创建的过程：

- 每一个 broker 启动的时候，它会在 Zookeeper 的 /brokers/ids 路径下创建一个 临时节点，并将自己的 broker.id 写入，从而将自身注册到集群；
- 当有多个 broker 时，所有 broker 会竞争性地在 Zookeeper 上创建 /controller 节点，由于 Zookeeper 上的节点不会重复，所以必然只会有一个 broker 创建成功，此时该 broker 称为 controller broker。它除了具备其他 broker 的功能外，**还负责管理主题分区及其副本的状态**。
- 当 broker 出现宕机或者主动退出从而导致其持有的 Zookeeper 会话超时时，会触发注册在 Zookeeper 上的 watcher 事件，此时 Kafka 会进行相应的容错处理；如果宕机的是 controller broker 时，还会触发新的 controller 选举。

# Hbase：

hbase架构：

## 1、Hmaster

**功能：**

1) 监控RegionServer

2) 处理RegionServer故障转移

3) 处理元数据的变更

4) 处理region的分配或移除

5) 在空闲时间进行数据的负载均衡

6) 通过Zookeeper发布自己的位置给客户端

## **2、RegionServer**

**功能：**

1) 负责存储HBase的实际数据

2) 处理分配给它的Region

3) 刷新缓存到HDFS

4) 维护HLog

5) 执行压缩

6) 负责处理Region分片

**3、Zookeeper**

1. 保证任何时候，集群中只有一个 Master；
2. 存贮所有 Region 的寻址入口；
3. 实时监控 Region Server 的状态，将 Region Server 的上线和下线信息实时通知给 Master；
4. 存储 HBase 的 Schema，包括有哪些 Table，每个 Table 有哪些 Column Family 等信息。

### **3.2 组件间的协作**

HBase 使用 ZooKeeper 作为分布式协调服务来维护集群中的服务器状态。 Zookeeper 负责维护可用服务列表，并提供服务故障通知等服务：

- 每个 Region Server 都会在 ZooKeeper 上创建一个临时节点，Master 通过 Zookeeper 的 Watcher 机制对节点进行监控，从而可以发现新加入的 Region Server 或故障退出的 Region Server；
- 所有 Masters 会竞争性地在 Zookeeper 上创建同一个临时节点，由于 Zookeeper 只能有一个同名节点，所以必然只有一个 Master 能够创建成功，此时该 Master 就是主 Master，主 Master 会定期向 Zookeeper 发送心跳。备用 Masters 则通过 Watcher 机制对主 HMaster 所在节点进行监听；
- 如果主 Master 未能定时发送心跳，则其持有的 Zookeeper 会话会过期，相应的临时节点也会被删除，这会触发定义在该节点上的 Watcher 事件，使得备用的 Master Servers 得到通知。所有备用的 Master Servers 在接到通知后，会再次去竞争性地创建临时节点，完成主 Master 的选举。



**组件：**

**1) Write-Ahead logs**

HBase的修改记录，当对HBase读写数据的时候，数据不是直接写进磁盘，它会在内存中保留一段时间（时间以及数据量阈值可以设定）。但把数据保存在内存中可能有更高的概率引起数据丢失，为了解决这个问题，数据会先写在一个叫做Write-Ahead logfile的文件中，然后再写入内存中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。

**2) H****F****ile**

这是在磁盘上保存原始数据的实际的物理文件，是实际的存储文件。

**3) Store**

HFile存储在Store中，一个Store对应HBase表中的一个列族。 将行数据按照 Key\Values 的形式存储在文件系统上。

**4) MemStore**

写缓存。它存储尚未写入磁盘的新数据(也就是内存)，并会在数据写入磁盘之前对其进行排序。每个 Region 上的每个列族都有一个 MemStore。

**5) Region**

Hbase表的分片，HBase表会根据RowKey值被切分成不同的region存储在RegionServer中，在一个RegionServer中可以有多个不同的region。 HBase Table 中的所有行按照 Row Key 的字典序排列。HBase Tables 通过行键的范围 (row key range) 被水平切分成多个 Region, 一个 Region 包含了在 start key 和 end key 之间的所有行。 每个表一开始只有一个 Region，随着数据不断增加，Region 会不断增大，当增大到一个阀值的时候，Region 就会等分为两个新的 Region。当 Table 中的行不断增多，就会有越来越多的 Region。

 **6)** **BlockCache:** 读缓存。它将频繁读取的数据存储在内存中，如果存储不足，它将按照 最近最少使用原则 清除多余的数据。

hbase的读写流程：

**写数据：**

1. Client向HregionServer发送写请求；
2. HRegionServer将数据写到HLog（write ahead log）。为了数据的持久化和恢复；
3. HRegionServer将数据写到内存（MemStore）；
4. 反馈Client写成功。

**读数据：**

1. Client先访问zookeeper，从meta表读取region的位置，然后读取meta表中的数据。meta中又存储了用户表的region信息；
2. 根据namespace、表名和rowkey在meta表中找到对应的region信息；
3. 找到这个region对应的regionserver；
4. 查找对应的region；
5. 先从MemStore找数据，如果没有，再到BlockCache里面读；
6. BlockCache还没有，再到StoreFile上读(为了读取的效率)；
7. 如果是从StoreFile里面读取的数据，不是直接返回给客户端，而是先写入BlockCache，再返回给客户端。

HBASE的flush机制：

Hbase的flush过程主要包括三个阶段：prepareFlush、flushCache、commit。

**prepaFlush：**对memstore做一个snapshot，同时为防止其他线程更新memstore，这里会为memstore加上一个排他锁，阻碍客户端的写操作，由于创建snapshot非常快。

**flushCache:**将上一个阶段产生的快照持久化到hdfs上的一个hfile存放位置为对应region文件夹下的.tmp目录。

**commit:**将上一个阶段产生的临时文件移动到对应的列簇目录下，同时删除第一部的snapshot。

1.（hbase.regionserver.global.memstore.size）默认;堆大小的40%

regionServer的全局memstore的大小，超过该大小会触发flush到磁盘的操作,默认是堆大小的40%,而且regionserver级别的flush会阻塞客户端读写

2.（hbase.hregion.memstore.flush.size）默认：128M

单个region里memstore的缓存大小，超过那么整个HRegion就会flush,

3.（hbase.regionserver.optionalcacheflushinterval）默认：1h

内存中的文件在自动刷新之前能够存活的最长时间

4.（hbase.regionserver.global.memstore.size.lower.limit）默认：堆大小 * 0.4 * 0.95

有时候集群的“写负载”非常高，写入量一直超过flush的量，这时，我们就希望memstore不要超过一定的安全设置。在这种情况下，写操作就要被阻塞一直到memstore恢复到一个“可管理”的大小, 这个大小就是默认值是堆大小 * 0.4 * 0.95，也就是当regionserver级别的flush操作发送后,会阻塞客户端写,一直阻塞到整个regionserver级别的memstore的大小为 堆大小 * 0.4 *0.95为止

5.（hbase.hregion.preclose.flush.size）默认为：5M

当一个 region 中的 memstore 的大小大于这个值的时候，我们又触发了region的 close时，会先运行“pre-flush”操作，清理这个需要关闭的memstore，然后 将这个 region 下线。当一个 region 下线了，我们无法再进行任何写操作。 如果一个 memstore 很大的时候，flush  操作会消耗很多时间。"pre-flush" 操作意味着在 region 下线之前，会先把 memstore 清空。这样在最终执行 close 操作的时候，flush 操作会很快。

6.（[hbase.hstore.compactionThreshold](http://hbase.hstore.compactionthreshold/)）默认：超过3个

一个store里面允许存的hfile的个数，超过这个个数会被写到新的一个hfile里面 也即是每个region的每个列族对应的memstore在flush为hfile的时候，默认情况下当超过3个hfile的时候就会对这些文件进行合并重写为一个新文件，设置个数越大可以减少触发合并的时间，但是每次合并的时间就会越长

HBASE的合并机制：

1. 当数据块达到3块，Hmaster触发合并操作，Region将数据块加载到本地，进行合并；
2. 当合并的数据超过256M，进行拆分，将拆分后的Region分配给不同的HregionServer管理；
3. 当HregionServer宕机后，将HregionServer上的hlog拆分，然后分配给不同的HregionServer加载，修改.META.；
4. 注意：HLog会同步到HDFS。

HBASE的split机制：

当Region达到阈值，会把过大的Region一分为二。

默认一个HFile达到10Gb的时候就会进行切分

hbase的hbase-default.xml参数：

**Flush:**

<!--当memstore的大小超过这个值的时候，会flush到磁盘。128M-->

<property>

<name>hbase.hregion.memstore.flush.size</name>

<value>134217728</value>

</property>

<!--单个regionserver的全部memstore的最大值。超过这个值总容量(Max Heap=983.4 M)*0.4，

一个新的put插入操作会被挂起，强制执行flush操作。 -->

<property>

<name>hbase.regionserver.global.memstore.upperLimit</name>

<value>0.4</value>

</property>

<!--当强制执行flush操作的时候，当低于这个值的时候，flush会停止。默认是堆大小的 35% . -->

<property>

<name>hbase.regionserver.global.memstore.lowerLimit</name>

<value>0.35</value>

</property>

**Compact:**

<!--当一个HStore含有多于这个值的HStoreFiles(每一个memstore flush产生一个HStoreFile)的时候，会执行一个合并操作，把这HStoreFiles写成一个-->

<property>

<name>[hbase.hstore.compactionThreshold](http://hbase.hstore.compactionthreshold/)</name>

<value>3</value>

</property>

<!--一个Region中的所有HStoreFile的major compactions的时间间隔。默认是1天。-->

<property>

<name>hbase.hregion.majorcompaction</name>

<value>86400000</value>

</property>

**Split:**

<!--最大HStoreFile大小。若某个列族的HStoreFile增长达到这个值，这个Hegion会被切割成两个。 默认: 10G.-->

<property>

<name>hbase.hregion.max.filesize</name>

<value>10737418240</value>

</property>

rowkey的设计原则：

**1.长度原则：**rowkey是一个二进制码流，可以是任意字符串，最大长度64kb，实际应用中一般为10-100bytes,以byte[]形式保存，一般设计定长。**建议越短越好，不要超过16个字节。**

**原因：**

- 数据的持久化文件HFile中是按照KeyValue存储的，如果rowkey过长 ，比如超过100字节，1000w行数据，光rowkey就要占用100*1000w=10亿个字节，将近1G数据，这样会极大影响HFile的存储效率；
- MemStore将缓存部分数据到内存，如果rowkey字段过长，内存的有效利用率就会降低，系统不能缓存更多的数据，这样会降低检索效率。

### **2** **rowkey****散列原则**

如果rowkey按照时间戳的方式递增，不要将时间放在二进制码的前面，建议将rowkey的高位作为散列字段，由程序随机生成，低位放时间字段，这样将提高数据均衡分布在每个RegionServer，以实现负载均衡的几率。如果没有散列字段，首字段直接是时间信息，所有的数据都会集中在一个RegionServer上，这样在数据检索的时候负载会集中在个别的RegionServer上，造成热点问题，会降低查询效率。

### **3** **rowkey****唯一原则**

必须在设计上保证其唯一性，rowkey是按照字典顺序排序存储的，因此，设计rowkey的时候，要充分利用这个排序的特点，将经常读取的数据存储到一块，将最近可能会被访问的数据放到一块。



**hbase的热点问题：**

HBase中的行是按照rowkey的字典顺序排序的，这种设计优化了scan操作，可以将相关的行以及会被一起读取的行存取在临近位置，便于scan。然而糟糕的rowkey设计是热点的源头。

热点发生在大量的client直接访问集群的一个或极少数个节点（访问可能是读，写或者其他操作）。大量访问会使热点region所在的单个机器超出自身承受能力，引起性能下降甚至region不可用，这也会影响同一个RegionServer上的其他region，由于主机无法服务其他region的请求。

设计良好的数据访问模式以使集群被充分，均衡的利用。为了避免写热点，设计rowkey使得不同行在同一个region，但是在更多数据情况下，数据应该被写入集群的多个region，而不是一个。下面是一些常见的避免热点的方法以及它们的优缺点：

**1.加盐****：**这里所说的加盐不是密码学中的加盐，而是在rowkey的前面增加随机数，具体就是给rowkey分配一个随机前缀以使得它和之前的rowkey的开头不同。分配的前缀种类数量应该和你想使用数据分散到不同的region的数量一致。加盐之后的rowkey就会根据随机生成的前缀分散到各个region上，以避免热点。



**2.哈希：**哈希会使同一行永远用一个前缀加盐。哈希也可以使负载分散到整个集群，但是读却是可以预测的。使用确定的哈希可以让客户端重构完整的rowkey，可以使用get操作准确获取某一个行数据。



**3.反转：**第三种防止热点的方法时反转固定长度或者数字格式的rowkey。这样可以使得rowkey中经常改变的部分（最没有意义的部分）放在前面。这样可以有效的随机rowkey，但是牺牲了rowkey的有序性。反转rowkey的例子以手机号为rowkey，可以将手机号反转后的字符串作为rowkey，这样的就避免了以手机号那样比较固定开头导致热点问题

Phoenix 的二级索引：

**Hbase的二级索引本质就是建立各列值与行键之间的映射关系。(** 当要对F:C1这列建立索引时，只需要建立F:C1各列值到其对应行键的映射关系，如C11->RK1等，这样就完成了对F:C1列值的二级索引的构建，当要查询符合F:C1=C11对应的F:C2的列值时即根据C1=C11来查询C2的值**)**



**其查询步骤如下:**



\1. 根据C1=C11到索引数据中查找其对应的RK，查询得到其对应的RK=RK1

\2. 得到RK1后就自然能根据RK1来查询C2的值了 这是构建二级索引大概思路，其他组合查询的联合索引的建立也类似。



在Hbase中，只有一个单一的按照字典序排序的rowKey索引，当使用rowKey来进行数据查询的时候速度较快，但是如果不使用rowKey来查询的话就会使用filter来对全表进行扫描，很大程度上降低了检索性能。而Phoenix提供了二级索引技术来应对这种使用rowKey之外的条件进行检索的场景。



Phoenix支持两种类型的索引技术：Global Indexing和Local Indexing



**Global Indexing:** 适用于多读少写的业务场景。使用

Hbase预分区：

Hbase在表刚刚创建的时候，只有一个分区（regin），当一个regin过大时候，表会split，分裂成2个分区，表在 进行split的时候会耗费大量的资源，所以hbase提供了预分区功能，即用户可以在创建表的时候对表按照一定的规则分区。

**如何进行预分区：**每一个region维护着startRow与endRow，如果加入的数据符合某个region维护的rowKey范围，则数据交给这个region维护。

Hbase的region太大或太小的问题：

**region太小：**更容易触发split，系统的整体访问服务会出现不稳定的现象。

**region太大：**由于长期得不到split，因此同一个region内的storefile数量太多，发生多次compaction压缩的机会增大，会降低系统的性能和稳定性。

**因此，region大小一般在5-10G大小，数量在20-200之间，一般是100个。**

Hbase的瓶劲问题：

1、不能支持条件查询，只支持Row Key来查询。

2、暂时不能支持Master Server 的故障切换，当masster宕机，整个储存系统就会挂掉

3、磁盘IO瓶颈限制，数据储存在磁盘上，不如直接储存在内存中的redis读写快。

Hbase如何导入数据：

使用MapReduce Job方式，根据Hbase API编写java脚本，将文本文件用文件流的方式截取，然后存储到多个字符串数组中，在put方法下，通过对表中的列族进行for循环遍历列名，用if判断列名后进行for循环调用put.add的方法对列族下每一个列进行设值，每个列族下有几个了就赋值几次！没有表先对先创建表。

Hbase和Hive的区别：

**共同点：**

1.hbase与hive都是架构在hadoop之上的。都是用hadoop作为底层存储

**区别：**

2.Hive是建立在Hadoop之上为了减少MapReducejobs编写工作的批处理系统，HBase是为了支持弥补Hadoop对实时操作的缺陷的项目 。

3.想象你在操作RMDB数据库，如果是全表扫描，就用Hive+Hadoop,如果是索引访问，就用HBase+Hadoop 。

4.Hive query就是MapReduce jobs可以从5分钟到数小时不止，

HBase是非常高效的，肯定比Hive高效的多。

5.Hive本身不存储和计算数据，它完全依赖于HDFS和MapReduce，Hive中的表纯逻辑。

6.hive借用hadoop的MapReduce来完成一些hive中的命令的执行

7.hbase是物理表，不是逻辑表，提供一个超大的内存hash表，搜索引擎通过它来存储索引，方便查询操作。

8.hbase是列存储。

9.hdfs作为底层存储，hdfs是存放文件的系统，而Hbase负责组织文件。

10.hive需要用到hdfs存储文件，需要用到MapReduce计算框架。

行式数据库与列式数据库的区别：

**传统行式：**

1、数据是按行储存。

2、没有索引的查询使用大量I/O。

3、建立索引和物化视图需要大量时间和资源。

4、面对查询的需求，数据库必须被大量膨胀才能满足性能的要求。

**列式：**

1、数据按列存储。

2、数据即使索引。

3、只访问查询涉及的列**（大量降低 系统的I/O）**。

4、每一列有一个线索来处理。（**查询的并发处理**）。

5、数据类型一致，数据特征相似。（**压缩高效**）。

列簇怎么创建比较好？（<=2）

rowKey最好要创建有规则的rowKey，即最好是有序的。HBase中一张表最好只创建一到两个列簇比较好，因为HBase不能很好的处理多个列簇。



#### 描述Hbase中scan和get的功能以及实现的异同.:

1.按指定RowKey 获取唯一一条记录，get方法（org.apache.hadoop.hbase.client.Get）Get 的方法处理分两种 : 设置了ClosestRowBefore 和没有设置的rowlock .主要是用来保证行的事务性，即每个get 是以一个row 来标记的.一个row中可以有很多family 和column.

2.按指定的条件获取一批记录，scan方法(org.apache.Hadoop.hbase.client.Scan)实现条件查询功能使用的就是scan 方式.

1)scan 可以通过setCaching 与setBatch 方法提高速度(以空间换时间)；   2)scan 可以通过setStartRow 与setEndRow 来限定范围([start，end]start 是闭区间，end 是开区间)。范围越小，性能越高。

3)scan 可以通过setFilter 方法添加过滤器，这也是分页、多条件查询的基础。 3.全表扫描，即直接扫描整张表中所有行记录

HBASE中compact用途是什么，什么时候触发，分为哪两种,有什么区别:

在hbase中每当有memstore数据flush到磁盘之后，就形成一个storefile，当storeFile的数量达到一定程度后，就需要将 storefile 文件来进行 compaction 操作。

Compact 的作用：

1>.合并文件

2>.清除过期，多余版本的数据

3>.提高读写数据的效率

HBase 中实现了两种 compaction 的方式：minor and major. 这两种 compaction 方式的区别是：

1、Minor 操作只用来做部分文件的合并操作以及包括 minVersion=0 并且设置 ttl 的过

期版本清理，不做任何删除数据、多版本数据的清理工作。

2、Major 操作是对 Region 下的HStore下的所有StoreFile执行合并操作，最终的结果是整理合并出一个文件。

#### **简述****Hbase filter的实现原理是什么？结合实际项目经验，写出几个使用filter的场景:**

HBase为筛选数据提供了一组过滤器，通过这个过滤器可以在HBase中的数据的多个维度（行，列，数据版本）上进行对数据的筛选操作，也就是说过滤器最终能够筛选的数据能够细化到具体的一个存储单元格上（由行键，列名，时间戳定位）。RowFilter、PrefixFilter。。。

hbase的filter是通过scan设置的，所以是基于scan的查询结果进行过滤.

过滤器的类型很多，但是可以分为两大类——比较过滤器，专用过滤器

过滤器的作用是在服务端判断数据是否满足条件，然后只将满足条件的数据返回给客户端；

如在进行订单开发的时候，我们使用rowkeyfilter过滤出某个用户的所有订单

hbase的储存结构的优缺点：

**Hbase的优点及应用场景**:

1. 半结构化或非结构化数据: 对于数据结构字段不够确定或杂乱无章非常难按一个概念去进行抽取的数据适合用HBase，因为HBase支持动态添加列。
2. 记录很稀疏： RDBMS的行有多少列是固定的。为null的列浪费了存储空间。HBase为null的Column不会被存储，这样既节省了空间又提高了读性能。
3. 多版本号数据： 依据Row key和Column key定位到的Value能够有随意数量的版本号值，因此对于须要存储变动历史记录的数据，用HBase是很方便的。比方某个用户的Address变更，用户的Address变更记录也许也是具有研究意义的。
4. 仅要求最终一致性： 对于数据存储事务的要求不像金融行业和财务系统这么高，只要保证最终一致性就行。（比如HBase+elasticsearch时，可能出现数据不一致）
5. 高可用和海量数据以及很大的瞬间写入量： WAL解决高可用，支持PB级数据，put性能高 适用于插入比查询操作更频繁的情况。比如，对于历史记录表和日志文件。（HBase的写操作更加高效）
6. 业务场景简单： 不需要太多的关系型数据库特性，列入交叉列，交叉表，事务，连接等。



**Hbase的缺点：**

1. 单一RowKey固有的局限性决定了它不可能有效地支持多条件查询
2. 不适合于大范围扫描查询
3. 不直接支持 SQL 的语句查询



## HMaster宕机的时候,哪些操作还能正常工作

对表内数据的增删查改是可以正常进行的,因为hbase client 访问数据只需要通过 zookeeper 来找到 rowkey 的具体 region 位置即可. 但是对于创建表/删除表等的操作就无法进行了,因为这时候是需要HMaster介入, 并且region的拆分,合并,迁移等操作也都无法进行了



# Sqoop

### **1** 、Sqoop导入数据到hdfs中的参数

/opt/module/sqoop/bin/sqoop import \

--connect \ # 特殊的jdbc连接的字符串

--username \

--password \

--target-dir \  # hdfs目标的目录

--delete-target-dir \ # 导入的目标目录如果存在则删除那个目录

--num-mappers \   #相当于 -m ,并行导入时map task的个数

--fields-terminated-by   \

--query "$2"  ' and $CONDITIONS;' # 指定满足sql和条件的数据导入

###  **2、**Sqoop导入hive时的参数

一步将表结构和数据都导入到hive中

bin/sqoop import \

--connect jdbc的url字符串 \

--table mysql中的表名\

--username 账号 \

--password 密码\

--hive-import \

--m mapTask的个数\

--hive-database hive中的数据库名;

### 

### **3、** **Sqoop导入导出Null存储一致性问题**

Hive中的Null在底层是以“\N”来存储，而MySQL中的Null在底层就是Null，为了保证数据两端的一致性,转化的过程中遇到null-string,null-non-string数据都转化成指定的类型，通常指定成"\N"。在导出数据时采用–input-null-string “\N” --input-null-non-string “\N” 两个参数。导入数据时采用–null-string “\N” --null-non-string “\N”。



###  **4、**Sqoop数据导出一致性问题

1）场景1：如Sqoop在导出到Mysql时，使用4个Map任务，过程中有2个任务失败，那此时MySQL中存储了另外两个Map任务导入的数据，此时老板正好看到了这个报表数据。而开发工程师发现任务失败后，会调试问题并最终将全部数据正确的导入MySQL，那后面老板再次看报表数据，发现本次看到的数据与之前的不一致，这在生产环境是不允许的。



使用—staging-table选项，将hdfs中的数据先导入到辅助表中，当hdfs中的数据导出成功后，辅助表中的数据在一个事务中导出到目标表中（也就是说这个过程要不完全成功，要不完全失败）。



为了能够使用staging这个选项，staging表在运行任务前或者是空的，要不就使用—clear-staging-table配置，如果staging表中有数据，并且使用了—clear-staging-table选项,sqoop执行导出任务前会删除staging表中所有的数据。



注意：–direct导入时staging方式是不可用的，使用了—update-key选项时staging方式也不能用。

​    **sqoop export \**

​    **--connect url \**

​    **--username root \**

​    **--password 123456 \**

​    **--table app_cource_study_report \**

​    **--columns watch_video_cnt,complete_video_cnt,dt \**

​    **--fields-terminated-by "\t" \**

​    **--export-dir "/user/hive/warehouse/tmp.db/app_cource_study_analysi_${day}" \**

​    **--staging-table app_cource_study_report_tmp \**

​    **--clear-staging-table \**

​    **--input-null-string '\\N' \**

​    **--null-non-string "\\N"**

2）场景2：设置map数量为1个（不推荐，面试官想要的答案不只这个）

多个Map任务时，采用–staging-table方式，仍然可以解决数据一致性问题。



### **5、** **Sqoop底层运行的任务是什么**

只有Map阶段，没有Reduce阶段的任务。



### **6、** **Map task并行度设置大于1的问题**

那么就是说当map task并行度大于1时，下面两个参数要同时使用

–split-by id 指定根据id字段进行切分

–m n 指定map并行度n个

### **7、** **Sqoop数据导出的时候一次执行多长时间**

Sqoop任务5分钟-2个小时的都有。取决于数据量。





# Hive：

Hive和数据库的比较：

Hive 和数据库除了拥有类似的查询语言，再无类似之处。

1. 数据存储位置

Hive存储在HDFS。数据库将数据保存在块设备或者本地文件系统中。

1. 数据更新

Hive中不建议对数据的改写。而数据库中的数据通常是需要经常进行修改的，

1. 执行延迟

Hive执行延迟较高。数据库的执行延迟较低。当然，这个是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候，Hive的并行计算显然能体现出优势。

1. 数据规模

Hive支持很大规模的数据计算；数据库可以支持的数据规模较小。

Hive的四个By的区别：

1. Sort By：分区内有序；
2. Order By：全局排序，只有一个Reducer；
3. Distrbute By：类似MR中Partition，进行分区，结合sort by使用。
4. Cluster By：当Distribute by和Sorts by字段相同时，可以使用Cluster by方式。Cluster by除了具有Distribute by的功能外还兼具Sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。

hive数据倾斜问题：

倾斜原因： map输出数据按Key Hash分配到reduce中,由于key分布不均匀、或者业务数据本身的特点。等原因造成的reduce上的数据量差异过大。



1.1)key分布不均匀



1.2)业务数据本身的特性



1.3)SQL语句造成数据倾斜



解决方案：



1>参数调节：



​    hive.map.aggr=true



​    hive.groupby.skewindata=true



有数据倾斜的时候进行负载均衡，当选项设定为true,生成的查询计划会有两个MR Job。第一个MR Job中，Map的输出结果集合会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MR Job在根据预处理的数据结果按照 Group By Key 分布到Reduce中(这个过程可以保证相同的 Group By Key 被分布到同一个Reduce中)，最后完成最终的聚合操作。



2>SQL语句调节：



   1)选用join key 分布最均匀的表作为驱动表。做好列裁剪和filter操作，以达到两表join的时候，数据量相对变小的效果。



   2)大小表Join： 使用map join让小的维度表（1000条以下的记录条数）先进内存。在Map端完成Reduce。



   3)大表Join大表：把空值的Key变成一个字符串加上一个随机数，把倾斜的数据分到不同的reduce上，由于null值关联不上，处理后并不影响最终的结果。



   4)count distinct大量相同特殊值：count distinct时，将值为空的情况单独处理，如果是计算count distinct，可以不用处理，直接过滤，在做后结果中加1。如果还有其他计算，需要进行group by，可以先将值为空的记录单独处理，再和其他计算结果进行union.

**分区和分桶的区别**

**分区**

是指按照数据表的某列或某些列分为多个区，**区从形式上可以理解为文件夹**，比如我们要收集某个大型网站的日志数据，一个网站每天的日志数据存在同一张表上，由于每天会生成大量的日志，导致数据表的内容巨大，在查询时进行全表扫描耗费的资源非常多。

那其实这个情况下，我们可以按照日期对数据表进行分区，不同日期的数据存放在不同的分区，在查询时只要指定分区字段的值就可以直接从该分区查找

**分桶**

分桶是相对分区进行更细粒度的划分。

**分桶将整个数据内容安装某列属性值得hash值进行区分（根据行）**，如要按照name属性分为3个桶，就是对name属性值的hash值对3取摸，按照取模结果对数据分桶。

如取模结果为0的数据记录存放到一个文件，取模为1的数据存放到一个文件，取模为2的数据存放到一个文件

# Redis：

redis的五种数据类型：

（1）字符串类型--string

（2）散列类型--hash

（3）列表类型--list

（4）集合类型--set

（5）有序集合类型--zset

redis缓存雪崩：

**由于原有缓存失效，新缓存未到期间**(例如：我们设置缓存时采用了相同的过期时间，在同一时刻出现大面积的缓存过期)，所有原本应该访问缓存的请求都去查询数据库了，而对数据库CPU和内存造成巨大压力，严重的会造成数据库宕机。从而形成一系列连锁反应，造成整个系统崩溃。

**解决方案是**：给每一个缓存数据增加相应的缓存标记，记录缓存的是否失效，如果缓存标记失效，则更新数据缓存

**1、缓存标记：记录缓存数据是否过期，如果过期会触发通知另外的线程在后台去更新实际key的缓存；**

**2、缓存数据：它的过期时间比缓存标记的时间延长1倍，例：标记缓存时间30分钟，数据缓存设置为60分钟。 这样，当缓存标记key过期后，实际缓存还能把旧数据返回给调用端，直到另外的线程在后台更新完成后，才会返回新缓存**。

**redis缓存穿透：**

是指用户查询的数据在数据库中没有，在缓存中也没有，导致用户查询的时候再缓存中找不到，每次都去数据库再查询一遍，然后返回空（相当于进行了两次无用的查询）。这样请求就绕过缓存直接查数据库，这也是经常提的缓存命中率问题。

有很多种方法可以有效地解决缓存穿透问题，最常见的则是采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。

**redis缓存预热：**

就是系统上线后，将相关的缓存数据直接加载到缓存系统。这样就可以避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题！用户直接查询事先被预热的缓存数据！

**解决思路：**

1、直接写个缓存刷新页面，上线时手工操作下；

2、数据量不大，可以在项目启动的时候自动进行加载；

3、定时刷新缓存；

**缓存更新：**

除了缓存服务器自带的缓存失效策略之外（Redis默认的有6种策略可供选择），我们还可以根据具体的业务需求进行自定义的缓存淘汰，常见的策略有两种：

1. 定时去清理过期的缓存；
2. 当有用户请求过来时，再判断这个请求所用到的缓存是否过期，过期的话就去底层系统得到新数据并更新缓存。

两者各有优劣，第一种的缺点是维护大量缓存的key是比较麻烦的，第二种的缺点就是每次用户请求过来都要判断缓存失效，逻辑相对比较复杂！具体用哪种方案，大家可以根据自己的应用场景来权衡。

**缓存降级：**

当访问量剧增、服务出现问题（如响应时间慢或不响应）或非核心服务影响到核心流程的性能时，仍然需要保证服务还是可用的，即使是有损服务。系统可以根据一些关键数据进行自动降级，也可以配置开关实现人工降级。

降级的最终目的是保证核心服务可用，即使是有损的。而且有些服务是无法降级的（如加入购物车、结算）。

在进行降级之前要对系统进行梳理，看看系统是不是可以丢卒保帅；从而梳理出哪些必须誓死保护，哪些可降级；比如可以参考日志级别设置预案：

1、一般：比如有些服务偶尔因为网络抖动或者服务正在上线而超时，可以自动降级

1. 2、警告：有些服务在一段时间内成功率有波动（如在95~100%之间），可以自动降级或人工降级，并发送告警；

2. 3、错误：比如可用率低于90%，或者数据库连接池被打爆了，或者访问量突然猛增到系统能承

   受的最大阀值，此时可以根据情况自动降级或者人工降级；

4、严重错误：比如因为特殊原因数据错误了，此时需要紧急人工降级。

**redis的哨兵模式：**

哨兵模式是一种特殊模式，当主服务器宕机后，把一台从服务器切换成主服务器，redis提供了哨兵命令，是一个独立的进程，会独立运行。原理是哨兵通过发送命令， 等待redis服务器响应，从而监控运行的多个redis实例。

**然而一个哨兵进程对****Redis服务器进行监控，可能会出现问题，为此，我们可以使用多个哨兵进行监控。各个哨兵之间还会进行监控，这样就形成了多哨兵模式。**

**用文字描述一下故障切换（****failover）的过程。假设主服务器宕机，哨兵1先检测到这个结果，系统并不会马上进行failover过程，仅仅是哨兵1主观的认为主服务器不可用，这个现象成为主观下线。当后面的哨兵也检测到主服务器不可用，并且数量达到一定值时，那么哨兵之间就会进行一次投票，投票的结果由一个哨兵发起，进行failover操作。切换成功后，就会通过发布订阅模式，让各个哨兵把自己监控的从服务器实现切换主机，这个过程称为客观下线。这样对于客户端而言，一切都是透明的。**

**redis的持久化：**

RDB持久化（快照）：



1. 1. 在指定的时间间隔内持久化
   2. 服务shutdown会自动持久化
   3. 输入bgsave也会持久化（ Redis进程执行fork操作创建子进程，RDB持久化过程由子 进程负责，完成后自动结束。阻塞只发生在fork阶段，一般时间很短）

AOF持久化（以日志形式记录每个更新操作）：

Redis重新启动时读取这个文件，重新执行新建、修改数据的命令恢复数据。

**保存策略：**

推荐（并且也是默认）的措施为每秒持久化一次，这种策略可以兼顾速度和安全性。

**缺点：**

1. 1. 比起RDB占用更多的磁盘空间
   2. 恢复备份速度要慢
   3. 每次读写都同步的话，有一定的性能压力
   4. 存在个别Bug，造成恢复不能

**选择策略：**

官方推荐：

如果对数据不敏感，可以选单独用RDB；不建议单独用AOF，因为可能出现Bug;如果只是做纯内存缓存，可以都不用

redis悲观锁：

具有强烈的独占和排他特性。它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度，因此，在整个数据处理过程中，将数据处于锁定状态。悲观锁的实现，往往依靠数据库提供的锁机制（也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据）。

简单来说：执行操作前假设当前的操作肯定（或有很大几率）会被打断（悲观）。基于这个假设，我们在做操作前就会把相关资源锁定，不允许自己执行期间有其他操作干扰。

悲观锁的并发性能差，但是能保证不会发生脏数据的可能性小一点。

redis乐观锁：

执行操作前假设当前操作不会被打断（乐观）。基于这个假设，我们在做操作前不会锁定资源，万一发生了其他操作的干扰，那么本次操作将被放弃。Redis使用的就是乐观锁。

redis是单线程，为什么那么快：



1. 完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。
2. 数据结构简单，对数据操作也简单，Redis中的数据结构是专门进行设计的
3. 采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗
4. 使用多路I/O复用模型，非阻塞IO
5. 使用底层模型不同，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis直接自己构建了VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求。



HashMap的底层源码，数据结构：

# mysql:

in 和 not in 要尽量避免，因为1、效率低。2、容易出现问题，或查询结果有误。

解决方法;1、用exists 或者 not exists 代替。2、用 join 代替。

**如果有确定且有限的集合是，可以使用in 和 not in 如：in(0,1,2)。**



any表示[子查询](https://www.baidu.com/s?wd=子查询&tn=SE_PcZhidaonwhc_ngpagmjz&rsv_dl=gh_pc_zhidao)的集合里有任何一个满足就返回true，all表示全部都满足才返回true，显然他们是不同的用法和意义。比如你用age>any(15,16,22,21,17,18,19)来判断一个人年龄是否大于集合里的所有人就是错误的，这里应该用all。



**exists:**强调的是是否返回结果集，不要求返回什么。（**exists子句不在乎返回什么，而在乎是不是有结果集返回**）

**not exists：要排除一部分数据用到。**

**IN和Exists的区别:**

简单来说，前者是非相关子查询，子查询先执行，且只执行一次，执行完毕后将值传递给外层查询；后者是相关子查询，将外层查询的一个元组传递给内层查询，然后执行内层查询，外层查询根据返回的结果集得到满足条件的记录，重复这个过程直到外层查询的所有元组都处理完毕。

**非相关子查询**的执行不依赖与外部的查询。

执行过程：

（1）执行子查询，其结果不被显示，而是传递给外部查询，作为外部查询的条件使用。

（2）执行外部查询，并显示整个结果。　　

非相关子查询一般可以分为：返回单值的子查询和返回一个列表的子查询

**相关子查询**的执行依赖于外部查询。多数情况下是子查询的WHERE子句中引用了外部查询的表。

执行过程：

（1）从外层查询中取出一个元组，将元组相关列的值传给内层查询。

（2）执行内层查询，得到子查询操作的值。

（3）外查询根据子查询返回的结果或结果集得到满足条件的行。

（4）然后外层查询取出下一个元组重复做步骤1-3，直到外层的元组全部处理完毕。

**case when 的两种用法:**(多行转多列)

**1.  case 字段** 

​        **when 条件 then 结果** 

​        **else 结果** 

​    **end;** 

**2.  case** 

​        **when 条件 then 结果** 

​        **when 条件 then 结果** 

​        **else 结果** 

​    **end;** 

**当处理null字段是要用第二种方法,第一种方法查询的结果是错的.**

**union all:****(一行转多列):**合并时不会去除重复的数据，**union**会去重。

**RANK()、DENSE_RANK()以及ROW_NUMBER()这三个函数都是对****分过组的数据排序加序号，这三个函数又各自有区别** :

**ROW_NUMBER()  OVER    ([PARTITION BY Colums1]     ORDER BY colums2)**  

**DENSE_RANK()     OVER    ([PARTITION BY Colums1]     ORDER BY colums2)**  

**RANK()                  OVER    ([PARTITION BY colums1]      ORDER BY colums2)**

**区别:**

三个函数都是按照colums1分组内从1开始排序

​    ROW_NUMBER() 是没有重复值的排序(即使两条记录相同，序号也不重复的)，不会有同名次。

​    DENSE_RANK() 是连续的排序，两个第二名仍然跟着第三名。

​    RANK()       是跳跃排序，两个第二名下来就是第四名。